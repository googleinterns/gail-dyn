run Bullet training:
run Dart:
Create one single docker for bullet,dart,pytorch
Using one single urdf file hopper (box/mesh capsule)
use PyCharm with docker interpreter mainly for API completion
run python (with GUI) still in container

docker run --gpus=all -ti --rm -e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix -v ~/tmp/bullet-dart:/root/bullet-dart pydart2-docker:v0 /bin/bash
cd baselines
pip3 install -e .
cd ..

Hopper:
1. torque control
2. ts = 1/500s
3. bullet default dampings?
4. collsion detector
5. solver iter
6. floating dofs no damping
7. gravity


Misc:
https://github.com/iory/mjcf2urdf not very good...
https://docs.docker.com/engine/install/linux-postinstall/
add user to docker group
https://www.jetbrains.com/help/pycharm/using-docker-as-a-remote-interpreter.html#run
https://youtrack.jetbrains.com/issue/PY-33489?_ga=2.16363125.2125752459.1593460896-148870692.1593460896
* Add more constraint force to root

To match model files:
1. com & link pos & quat at 0,0,0,0
2. com & link pos & quat at random q
3. damping/friction
4. joint limit
5. mass values / moment of inertia
6. capsule shapes


added noise;
turned off state normalize

joint limit force
constraint root force
contact force

bullet issue:
TODO
DART issue:
TODO

Change box to capsule in Dart will not transfer
BUT, will work if change in Bullet... (2.5/3 policies)
Seems that only failed because get stuck in places where DART has some issue solving for capsules
Try mujoco??

lower friction

Unmatched aspects between dart and bullet:
1. bullet default numerical damping/ joint friction
2. moment of inertia (dart uses default calced from shape)
3. restitution...


Laikago:
1. toe_limit
2. Use inertia computed from shapes
3/ increase alive bouns
4. stop when torso hits rgound

soft vs hard reset: what are people's habits?


GAN-hopper:
1. include both q and dq for now in s_t, though dq is not in model from Yuxiang
2. Not including past info in s_t for now as well (no partial obs).
3. how to handle dx?
    - dx is actually already there...just not accurate
    - replace old dx with last ave dx in s_t/obs

Discriminator more likely to succeed when (s_t, ..., s_t+n) n becomes larger - should be a tunable param
Nevertheless, rollout & store traj part remains the same.



4. probably when rolling out model, should not always start from beginning

5. reward calc uses uncliped, unnoisy, unscaled obs/state
what should the training data look like? (s_t, a_t, s_t1)
probably should not be clipped.
ground truth is noisy anyways
** just get rid of clipping for now, policies seem to transfer still...

6. what about action clipping?
just use clipped a_t in f_theta seems fine.