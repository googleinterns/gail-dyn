run Bullet training:
run Dart:
Create one single docker for bullet,dart,pytorch
Using one single urdf file hopper (box/mesh capsule)
use PyCharm with docker interpreter mainly for API completion
run python (with GUI) still in container

docker run --gpus=all -ti --rm -e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix -v ~/tmp/bullet-dart:/root/bullet-dart pydart2-docker:v0 /bin/bash
cd baselines
pip3 install -e .
cd ..

Hopper:
1. torque control
2. ts = 1/500s
3. bullet default dampings?
4. collsion detector
5. solver iter
6. floating dofs no damping
7. gravity


Misc:
https://github.com/iory/mjcf2urdf not very good...
https://docs.docker.com/engine/install/linux-postinstall/
add user to docker group
https://www.jetbrains.com/help/pycharm/using-docker-as-a-remote-interpreter.html#run
https://youtrack.jetbrains.com/issue/PY-33489?_ga=2.16363125.2125752459.1593460896-148870692.1593460896
* Add more constraint force to root

To match model files:
1. com & link pos & quat at 0,0,0,0
2. com & link pos & quat at random q
3. damping/friction
4. joint limit
5. mass values / moment of inertia
6. capsule shapes


added noise;
turned off state normalize

joint limit force
constraint root force
contact force

bullet issue:
TODO
DART issue:
TODO

Change box to capsule in Dart will not transfer
BUT, will work if change in Bullet... (2.5/3 policies)
Seems that only failed because get stuck in places where DART has some issue solving for capsules
Try mujoco??

lower friction

Unmatched aspects between dart and bullet:
1. bullet default numerical damping/ joint friction
2. moment of inertia (dart uses default calced from shape)
3. restitution...


Laikago:
1. toe_limit
2. Use inertia computed from shapes
3/ increase alive bouns
4. stop when torso hits rgound

soft vs hard reset: what are people's habits?


GAN-hopper:
1. include both q and dq for now in s_t, though dq is not in model from Yuxiang
2. Not including past info in s_t for now as well (no partial obs).
3. how to handle dx?
    - dx is actually already there...just not accurate
    - replace old dx with last ave dx in s_t/obs

Discriminator more likely to succeed when (s_t, ..., s_t+n) n becomes larger - should be a tunable param
Nevertheless, rollout & store traj part remains the same.



4. probably when rolling out model, should not always start from beginning

5. reward calc uses uncliped, unnoisy, unscaled obs/state
what should the training data look like? (s_t, a_t, s_t1)
probably should not be clipped.
ground truth is noisy anyways
** just get rid of clipping for now, policies seem to transfer still...

6. what about action clipping?
just use clipped a_t in f_theta seems fine.

7. what is this weight clipping? might be in odds with pre-training..

8. Does it make sense to add obs noise when using/testing MB?
 - visualizing MB
 - using MB to train policy (maybe add noise is fine)

Need to train some oracles
Is action scale harder than action clip?

Seems that it did not really predict fall over..


GAN-hopper v2:

TODO:
https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/pull/215
refactor distrbutions
https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/pull/219/files?file-filters%5B%5D=.py#diff-1a0c504086a1f5d300c2ce3a75af2ef7
plot training reward

https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/issues/204
https://github.com/openai/baselines/blob/master/baselines/gail/adversary.py#L51
self.reward_op = -tf.log(1-tf.nn.sigmoid(generator_logits)+1e-8)
# seems no reward normalize

# TODO:
# gail always positive reward, does it favor longer episode?
# maybe should not early terminate, especially when training dynamics

# TODO:
# multi-cpu??
# GAIL seems slow, beacuse default params make Dis use very little data every iteration (5*128)
# versus PPO(Gen) can use large data every iteration (32*250), also run 10 epochs on same data
# if Dis can use more data per update, multi-cpu can make it faster (but might overfit to expert data)

# TODO: I changed discriminator optimizer

TODO
# maybe should use schocastic policy when train dynamics policy

# need a flag for standard vs dynamics gail

# mod expert data loader so that s contains (s_t,a_t) and "a" contains (s_t+1)
# seems that rollouts storage can be kept the same structure s_0..N, a_0..(N-1) already there
# when predict reward, r[step] = Dis(s[step] (which is s[step],a[step] in the standard setting), s[step+1] (first half))
    last step? no problem, rollout has s 1-longer than a
# mod rollout gen:
    add next state to batch generator

TODO:
maybe add some spatial constraint that no con f above certain height
TODO:
never predicts fall over - mod data collection early terminate
TODO:
better discriminator features, like delta s * 3
TODO:
check outputted action
TODO: check dis output when return=50
TODO: inconsistency of early termination
TODO:
constrain friction force dir

TODO: revert params

How to help dis:
1. use Ground truth (s,a)
2. use more data/traj
3. dis could use a longer window as input


Training schedule:
1. Train dis longer/more frequently
2. (later) interleave between pi & model improvement (ensemble)
3. add randomness to generator


verify that dis always predicts high reward for fake
verify that expert loss is low but fake loss is high

GAN-hopper v2.1:
Use longer window for dis input
0s -0.04s -0.08s -0.12s -0.16s

# need to change D input dim
# need a new expert loader
# currently simply split (s,a,s) into (s,a) and s

# need a new policy data generator as well

# need a new predict reward


Assign reward to beginning of traj?
pad initial state at the beginning


TODO: reproducibility issue

TODO: gail reward & loss stopped matching.. might be an issue

provide GT value seems to be important? entropy is more healthy
Maybe large a range is also important?
(Hopefully add a bit fall down data is not important

# gail ephoch matter??

# put a hold on longer input window
# try laikago with new obs space first
    # root z, rpy (exclude y for now, decide what to do with z later)
        # need z for dynamic markovian
    # base linear & *angular* vels (change ang vel to history of info later)
    # q and dq
    # the only change was y, TODO: exclude y, keep it for now
# TODO: think about root vel smoothing later
# TODO: remove contact bits

TODO: decrease min height for termination
check the list in slides

TODO: hardcoded dim in main.py

TODO: put reward calc in if, accelerate later on

# TODO: no moment arm for laika contacts for now.


# obs space
# env action space

# is it reasonable to assume behavior pi trained in sim will walk several steps before falling over
# in sim-to-real? (reason asking is that I need to tune stiffness for different policies to create such a scenario)


# choose to commit suicide
# turn off alive pen for now.


List of attempts:
1. --5mg -> 2mg
2. --turn off early termination (still blows everywhere)
3. more forgiving early termination
4. turn off D warm-start
5. reshape lateral action space
6. larger G & D

Generator too weak?
feedback from D too sparse?
# both seem to point to pre-training
# seem to be share functionality with GT metric

# during pre-train, rather than use gail reward, use discrepancy to sim s+ as reward
# Need this functionality:
    # initialize a different session?
    # simply reset robot to (s,a) with floor
    # roll one step to get s+
# GT metric, during testing of env
# does that agree with D output.


TODO: design some experiment to show whether the improvement comes from robust policy
(i.e. do we learn anything?)

# about the local minima of blowing up everything.

# generator dropout?




# need two figures comparing improvements before and after, finish weekly summary
# think about what the action noise changes
# A lot of tuning which we don't like:
    # strange thing about alive pen - if ave low at beginning, seems always low...
# finish pre-train and metric analysis


# TODO: check why the new data does not work..


Rand force 160/80/120(todo)
# the problem with this baseline is that it does not mimick softness at all
# different from randomization baseline
# 2 cases: both high and low(high force) reward during training fail in testing


bascially changed from tuning alive pen to gamma

maybe do need to learn a termination helper somehow



1. set up the metric (looks like the diff between trained hard dyn & hard floor is as large as hard floor & soft floor)
    large portion of L2 errors come from dq/10
    # TODO: usage of absolute x
2. new behavior policy / new gap
3. set up baselines: PPO fine-tine; random force; randomization

Gamma
“Fine-tune” policy using GAIL dynamics trained with a large hard-floor Source Env dataset.

4. reverse the metric rolling out to figure out why D thinks negatively
5. see if D matches q or dq distance
6. read rl-cycleGAN

Got some odd results: gail dyn learned on hard floor, by L2 per-step error, is closer to soft floor than to hard floor (both dyn)
; gail dyn learned on soft floor is farther away from soft than dyn learned on hard floor (much softer??)


dis at lower spatial freq than gen


# TODO: a list of things we throw away in the current pipeline
1. value function of behavior policy
2. value function of gail-dyn-policy
3. D of gail-dyn
4. value function of fine-tuned policy


Hard floor
D score ~0.73

soft floor
D score ~0.78

G
D score ~0.57



We would like to learn

compensate previous mistakes?

MPC?




Use longer window for dis input
0s -0.04s -0.08s -0.12s -0.16s

# need to change D input dim
# need a new expert loader
# currently simply split (s,a,s) into (s,a) and s

# need a new policy data generator as well

# need a new predict reward


Use longer time window for D input

Generate rollouts: append T-1 steps after each rollout (such that each episode at least T long & T+1 obs)
Appended state/ action, some placeholder state/action like all zeros
The D takes in (st, at, st+1,at+1,...st+T) as input
During reward assignment, for each (st,at,st+1), that is not in the appended part, give a reward predicted by D (st, at, st+1,at+1,...st+T)
This is in a way delayed reward
Do not use the tuples in appended part for Generator PPO update
Note, we cannot simply rollout G longer in step 1 since we cannot assign reward to the very last tuples
Note, this seems better than padding states at beginning of each rollout, as this forces Generator to match later observations from the beginning.

# convention for place-holder state: height=1, all else 0
# convention for place-holder action: all 0

# consts T (16, 0.32s)
#

# changes to expert collection:
bugfix: need to change current final s+ to placeholder state (sN-1, aN-1, sA) (length-N)
append placeholder states (a,s) for T(?) times to each traj
The last D input (sN-1, aN-1, sA, aA, ..., )        (a N+T times, s N+T times, s+ no longer needed actually)

# changes to expert loader:
They are already in features when loaded
for i=0:N-1:
    (no s here, since I do not need G to be correct each step)
    construct (s_i, a_i, a_i+4, a_i+8, a_i+12, s_i+16) (max s_N+15 (N+T-1))

# changes to policy loader (storage):
Need a new ff-generator:
After insert, go through all the stored (s,a, masks) to create D input for each
for i=0:n_steps(1000):
    obs is (s,a)

# changes to D update
no need to split policy state and policy action now


Seems like need a function
input:
    array of s, array of a, t, steps_until_terminate
output:
    st, at, at+4, ..., st+16


expert loader:
sut = len(traj) - t

policy loader:
add an array (num_steps, num_processes, 1) storing sut of each (s,a,r)
during insert,

How to handle half-traj at the end of each PPO interaction batch?
- rollout sim for additional 16 steps...





When calling batchSampler, only sample from the non-appended part

Problem with return calc/GAE?


1. Appending T steps to each expert traj or policy traj(upon reach done)
    if last done to num_steps < T, only append <T steps until num_steps
    the last T steps are also treated as appended (it is likely that all the T steps are normal steps)
    keep track of an array of real_state mask containing non-appended steps
(Need a function appending k placeholder steps (return array s, a (together obs)), which should be easy)




Use longer time window for D input (simpler version)

const look back window length T=8

1. no change of data - maybe fix the terminal state of expert/policy to place-holder
2. D takes in (st-8, at-8, at-4, at, st+1) as input
    expert loader:
        for each traj,
        once get all the (st, at, st+1), iterate from 9-th tuple to construct a new dataset
    policy loader:
        # TODO: no final obs for each traj, that reward value masked out as well
        # TODO: maybe add one step to each rollout...
        * A=torch.rand((2,3,2))
        * A.permute(1,0,2).reshape(-1,2) non-continuous layout
        maintain a matrix (num_steps, num_processes, 1) indicating the step idx of each tuple (s,a,r)
        get a binary matrix of the above indicating if each tuple has st-8 or not
        fill in (st-8, at-8, at-4) for every (st, at)
            starting from 8-th step, 0~7 all zeros (later trajs will be non-sense at the beginning)
            need new storage self.past_info, storage class take behavior_a_dim as input
        only sample from the idx with 1's
    overwriting rewards
        0 reward for 0~7 of each traj (i.e. *binary matrix)
    update policy ppo, unchanged.


Need a function takes in tensor matrix st (N+1) at (N) masks (N+1, offset from 1?),

first reshape to sequential order (2D)


# resetted obs align with done_mask 0 (step+1)
# which should align with rollout_step = 0


# retrain everything with movable base

# TODO: soft error should at least be temporal enxtended

# TODO: think of DeepMimic initialization

# try GAN? (multi-step)

# Does policy batch even matters??
expert 128, policy 1??
then what about expert 1, policy 1 (downsample 128)

this is not only downsample then, but also using tiny batch size to update


# difference between adaptive alive bonus and adaptive final reward


# Clip weights of discriminator


# TODO: GAE lambda higher might be better for sparse reward

# TODO: architecture: have one action that controls force or not.
