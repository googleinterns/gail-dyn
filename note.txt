run Bullet training:
run Dart:
Create one single docker for bullet,dart,pytorch
Using one single urdf file hopper (box/mesh capsule)
use PyCharm with docker interpreter mainly for API completion
run python (with GUI) still in container

docker run --gpus=all -ti --rm -e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix -v ~/tmp/bullet-dart:/root/bullet-dart pydart2-docker:v0 /bin/bash
cd baselines
pip3 install -e .
cd ..

Hopper:
1. torque control
2. ts = 1/500s
3. bullet default dampings?
4. collsion detector
5. solver iter
6. floating dofs no damping
7. gravity


Misc:
https://github.com/iory/mjcf2urdf not very good...
https://docs.docker.com/engine/install/linux-postinstall/
add user to docker group
https://www.jetbrains.com/help/pycharm/using-docker-as-a-remote-interpreter.html#run
https://youtrack.jetbrains.com/issue/PY-33489?_ga=2.16363125.2125752459.1593460896-148870692.1593460896
* Add more constraint force to root

To match model files:
1. com & link pos & quat at 0,0,0,0
2. com & link pos & quat at random q
3. damping/friction
4. joint limit
5. mass values / moment of inertia
6. capsule shapes


added noise;
turned off state normalize

joint limit force
constraint root force
contact force

bullet issue:
TODO
DART issue:
TODO

Change box to capsule in Dart will not transfer
BUT, will work if change in Bullet... (2.5/3 policies)
Seems that only failed because get stuck in places where DART has some issue solving for capsules
Try mujoco??

lower friction

Unmatched aspects between dart and bullet:
1. bullet default numerical damping/ joint friction
2. moment of inertia (dart uses default calced from shape)
3. restitution...


Laikago:
1. toe_limit
2. Use inertia computed from shapes
3/ increase alive bouns
4. stop when torso hits rgound

soft vs hard reset: what are people's habits?


GAN-hopper:
1. include both q and dq for now in s_t, though dq is not in model from Yuxiang
2. Not including past info in s_t for now as well (no partial obs).
3. how to handle dx?
    - dx is actually already there...just not accurate
    - replace old dx with last ave dx in s_t/obs

Discriminator more likely to succeed when (s_t, ..., s_t+n) n becomes larger - should be a tunable param
Nevertheless, rollout & store traj part remains the same.



4. probably when rolling out model, should not always start from beginning

5. reward calc uses uncliped, unnoisy, unscaled obs/state
what should the training data look like? (s_t, a_t, s_t1)
probably should not be clipped.
ground truth is noisy anyways
** just get rid of clipping for now, policies seem to transfer still...

6. what about action clipping?
just use clipped a_t in f_theta seems fine.

7. what is this weight clipping? might be in odds with pre-training..

8. Does it make sense to add obs noise when using/testing MB?
 - visualizing MB
 - using MB to train policy (maybe add noise is fine)

Need to train some oracles
Is action scale harder than action clip?

Seems that it did not really predict fall over..


GAN-hopper v2:

TODO:
https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/pull/215
refactor distrbutions
https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/pull/219/files?file-filters%5B%5D=.py#diff-1a0c504086a1f5d300c2ce3a75af2ef7
plot training reward

https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/issues/204
https://github.com/openai/baselines/blob/master/baselines/gail/adversary.py#L51
self.reward_op = -tf.log(1-tf.nn.sigmoid(generator_logits)+1e-8)
# seems no reward normalize

# TODO:
# gail always positive reward, does it favor longer episode?
# maybe should not early terminate, especially when training dynamics

# TODO:
# multi-cpu??
# GAIL seems slow, beacuse default params make Dis use very little data every iteration (5*128)
# versus PPO(Gen) can use large data every iteration (32*250), also run 10 epochs on same data
# if Dis can use more data per update, multi-cpu can make it faster (but might overfit to expert data)

# TODO: I changed discriminator optimizer

TODO
# maybe should use schocastic policy when train dynamics policy

# need a flag for standard vs dynamics gail

# mod expert data loader so that s contains (s_t,a_t) and "a" contains (s_t+1)
# seems that rollouts storage can be kept the same structure s_0..N, a_0..(N-1) already there
# when predict reward, r[step] = Dis(s[step] (which is s[step],a[step] in the standard setting), s[step+1] (first half))
    last step? no problem, rollout has s 1-longer than a
# mod rollout gen:
    add next state to batch generator

TODO:
maybe add some spatial constraint that no con f above certain height
TODO:
never predicts fall over - mod data collection early terminate
TODO:
better discriminator features, like delta s * 3
TODO:
check outputted action
TODO: inconsistency of early termination
TODO:
constrain friction force dir
