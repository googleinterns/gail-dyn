run Bullet training:
run Dart:
Create one single docker for bullet,dart,pytorch
Using one single urdf file hopper (box/mesh capsule)
use PyCharm with docker interpreter mainly for API completion
run python (with GUI) still in container

docker run --gpus=all -ti --rm -e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix -v ~/tmp/bullet-dart:/root/bullet-dart pydart2-docker:v0 /bin/bash
cd baselines
pip3 install -e .
cd ..

Hopper:
1. torque control
2. ts = 1/500s
3. bullet default dampings?
4. collsion detector
5. solver iter
6. floating dofs no damping
7. gravity


Misc:
https://github.com/iory/mjcf2urdf not very good...
https://docs.docker.com/engine/install/linux-postinstall/
add user to docker group
https://www.jetbrains.com/help/pycharm/using-docker-as-a-remote-interpreter.html#run
https://youtrack.jetbrains.com/issue/PY-33489?_ga=2.16363125.2125752459.1593460896-148870692.1593460896
* Add more constraint force to root

To match model files:
1. com & link pos & quat at 0,0,0,0
2. com & link pos & quat at random q
3. damping/friction
4. joint limit
5. mass values / moment of inertia
6. capsule shapes


added noise;
turned off state normalize

joint limit force
constraint root force
contact force

bullet issue:
TODO
DART issue:
TODO

Change box to capsule in Dart will not transfer
BUT, will work if change in Bullet... (2.5/3 policies)
Seems that only failed because get stuck in places where DART has some issue solving for capsules
Try mujoco??

lower friction

Unmatched aspects between dart and bullet:
1. bullet default numerical damping/ joint friction
2. moment of inertia (dart uses default calced from shape)
3. restitution...


Laikago:
1. toe_limit
2. Use inertia computed from shapes
3/ increase alive bouns
4. stop when torso hits rgound

soft vs hard reset: what are people's habits?


GAN-hopper:
1. include both q and dq for now in s_t, though dq is not in model from Yuxiang
2. Not including past info in s_t for now as well (no partial obs).
3. how to handle dx?
    - dx is actually already there...just not accurate
    - replace old dx with last ave dx in s_t/obs

Discriminator more likely to succeed when (s_t, ..., s_t+n) n becomes larger - should be a tunable param
Nevertheless, rollout & store traj part remains the same.



4. probably when rolling out model, should not always start from beginning

5. reward calc uses uncliped, unnoisy, unscaled obs/state
what should the training data look like? (s_t, a_t, s_t1)
probably should not be clipped.
ground truth is noisy anyways
** just get rid of clipping for now, policies seem to transfer still...

6. what about action clipping?
just use clipped a_t in f_theta seems fine.

7. what is this weight clipping? might be in odds with pre-training..

8. Does it make sense to add obs noise when using/testing MB?
 - visualizing MB
 - using MB to train policy (maybe add noise is fine)

Need to train some oracles
Is action scale harder than action clip?

Seems that it did not really predict fall over..


GAN-hopper v2:

TODO:
https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/pull/215
refactor distrbutions
https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/pull/219/files?file-filters%5B%5D=.py#diff-1a0c504086a1f5d300c2ce3a75af2ef7
plot training reward

https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/issues/204
https://github.com/openai/baselines/blob/master/baselines/gail/adversary.py#L51
self.reward_op = -tf.log(1-tf.nn.sigmoid(generator_logits)+1e-8)
# seems no reward normalize

# TODO:
# gail always positive reward, does it favor longer episode?
# maybe should not early terminate, especially when training dynamics

# TODO:
# multi-cpu??
# GAIL seems slow, beacuse default params make Dis use very little data every iteration (5*128)
# versus PPO(Gen) can use large data every iteration (32*250), also run 10 epochs on same data
# if Dis can use more data per update, multi-cpu can make it faster (but might overfit to expert data)

# TODO: I changed discriminator optimizer

TODO
# maybe should use schocastic policy when train dynamics policy

# need a flag for standard vs dynamics gail

# mod expert data loader so that s contains (s_t,a_t) and "a" contains (s_t+1)
# seems that rollouts storage can be kept the same structure s_0..N, a_0..(N-1) already there
# when predict reward, r[step] = Dis(s[step] (which is s[step],a[step] in the standard setting), s[step+1] (first half))
    last step? no problem, rollout has s 1-longer than a
# mod rollout gen:
    add next state to batch generator

TODO:
maybe add some spatial constraint that no con f above certain height
TODO:
never predicts fall over - mod data collection early terminate
TODO:
better discriminator features, like delta s * 3
TODO:
check outputted action
TODO: check dis output when return=50
TODO: inconsistency of early termination
TODO:
constrain friction force dir

TODO: revert params

How to help dis:
1. use Ground truth (s,a)
2. use more data/traj
3. dis could use a longer window as input


Training schedule:
1. Train dis longer/more frequently
2. (later) interleave between pi & model improvement (ensemble)
3. add randomness to generator


verify that dis always predicts high reward for fake
verify that expert loss is low but fake loss is high

GAN-hopper v2.1:
Use longer window for dis input
0s -0.04s -0.08s -0.12s -0.16s

# need to change D input dim
# need a new expert loader
# currently simply split (s,a,s) into (s,a) and s

# need a new policy data generator as well

# need a new predict reward


Assign reward to beginning of traj?
pad initial state at the beginning


TODO: reproducibility issue

TODO: gail reward & loss stopped matching.. might be an issue

provide GT value seems to be important? entropy is more healthy
Maybe large a range is also important?
(Hopefully add a bit fall down data is not important

# gail ephoch matter??

# put a hold on longer input window
# try laikago with new obs space first
    # root z, rpy (exclude y for now, decide what to do with z later)
        # need z for dynamic markovian
    # base linear & *angular* vels (change ang vel to history of info later)
    # q and dq
    # the only change was y, TODO: exclude y, keep it for now
# TODO: think about root vel smoothing later
# TODO: remove contact bits

TODO: decrease min height for termination
check the list in slides

TODO: hardcoded dim in main.py

TODO: put reward calc in if, accelerate later on

# TODO: no moment arm for laika contacts for now.


# obs space
# env action space

# is it reasonable to assume behavior pi trained in sim will walk several steps before falling over
# in sim-to-real? (reason asking is that I need to tune stiffness for different policies to create such a scenario)


# choose to commit suicide
# turn off alive pen for now.


List of attempts:
1. --5mg -> 2mg
2. --turn off early termination (still blows everywhere)
3. more forgiving early termination
4. turn off D warm-start
5. reshape lateral action space
6. larger G & D

Generator too weak?
feedback from D too sparse?
# both seem to point to pre-training
# seem to be share functionality with GT metric

# during pre-train, rather than use gail reward, use discrepancy to sim s+ as reward
# Need this functionality:
    # initialize a different session?
    # simply reset robot to (s,a) with floor
    # roll one step to get s+
# GT metric, during testing of env
# does that agree with D output.


TODO: design some experiment to show whether the improvement comes from robust policy
(i.e. do we learn anything?)

# about the local minima of blowing up everything.

# generator dropout?




# need two figures comparing improvements before and after, finish weekly summary
# think about what the action noise changes
# A lot of tuning which we don't like:
    # strange thing about alive pen - if ave low at beginning, seems always low...
# finish pre-train and metric analysis


# TODO: check why the new data does not work..


Rand force 160/80/120(todo)
# the problem with this baseline is that it does not mimick softness at all
# different from randomization baseline
# 2 cases: both high and low(high force) reward during training fail in testing


bascially changed from tuning alive pen to gamma

maybe do need to learn a termination helper somehow



1. set up the metric (looks like the diff between trained hard dyn & hard floor is as large as hard floor & soft floor)
    large portion of L2 errors come from dq/10
    # TODO: usage of absolute x
2. new behavior policy / new gap
3. set up baselines: PPO fine-tine; random force; randomization

Gamma
“Fine-tune” policy using GAIL dynamics trained with a large hard-floor Source Env dataset.

4. reverse the metric rolling out to figure out why D thinks negatively
5. see if D matches q or dq distance
6. read rl-cycleGAN

Got some odd results: gail dyn learned on hard floor, by L2 per-step error, is closer to soft floor than to hard floor (both dyn)
; gail dyn learned on soft floor is farther away from soft than dyn learned on hard floor (much softer??)


dis at lower spatial freq than gen







